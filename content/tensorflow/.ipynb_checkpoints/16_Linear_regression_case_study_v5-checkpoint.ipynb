{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Linear Regression Case Study\n",
    "author: Thomas\n",
    "date: '2018-08-29'\n",
    "slug: linear\n",
    "categories: []\n",
    "tags:\n",
    "  - intro_tf\n",
    "header:\n",
    "  caption: ''\n",
    "  image: ''\n",
    "---\n",
    "\n",
    "In this tutorial, you will learn how to check the data and prepare it to create a linear\n",
    "regression task.\n",
    "\n",
    "This tutorial is divided into two parts:\n",
    "\n",
    "-   Look for interaction\n",
    "\n",
    "-   Test the model\n",
    "\n",
    "In the previous tutorial, you used the Boston dataset to estimate the\n",
    "median price of a house. Boston dataset has a small size, with only 506\n",
    "observations. This dataset is considered as a benchmark to try new\n",
    "linear regression algorithms.\n",
    "\n",
    "The dataset is composed of:\n",
    "\n",
    "| Variable | Description                                                          |\n",
    "|----------|----------------------------------------------------------------------|\n",
    "| zn       | The proportion of residential land zoned for lots over 25,000 sq.ft. |\n",
    "| indus    | The proportion of non-retail business acres per town.                |\n",
    "| nox      | nitric oxides concentration                                          |\n",
    "| rm       | average number of rooms per dwelling                                 |\n",
    "| age      | the proportion of owner-occupied units built before 1940             |\n",
    "| dis      | weighted distances to five Boston employment centers                 |\n",
    "| tax      | full-value property-tax rate per dollars 10,000                      |\n",
    "| ptratio  | the pupil-teacher ratio by a town                                    |\n",
    "| medv     | The median value of owner-occupied homes in thousand dollars         |\n",
    "| crim     | per capita crime rate by town                                        |\n",
    "| chas     | Charles River dummy variable (1 if bounds river; 0 otherwise)        |\n",
    "| B        | the proportion of blacks by the town                                 |\n",
    "\n",
    "In this tutorial, we will estimate the median price using a linear\n",
    "regressor, but the focus is on one particular process of machine\n",
    "learning: \"data preparation.\"\n",
    "\n",
    "A model generalizes the pattern in the data. To capture such a pattern,\n",
    "you need to find it first. A good practice is to perform a data analysis\n",
    "before running any machine learning algorithm.\n",
    "\n",
    "**Choosing the right features makes all the difference in the success of\n",
    "your model. Imagine you try to estimate the wage of a people, if you do\n",
    "not include the gender as a covariate, you end up with a poor\n",
    "estimate.**\n",
    "\n",
    "Another way to improve the model is to look at the correlation between\n",
    "the independent variable. Back to the example, you can think of\n",
    "education as an excellent candidate to predict the wage but also the\n",
    "occupation. It is fair to say, the occupation depends on the level of\n",
    "education, namely higher education often leads to a better occupation.\n",
    "If we generalize this idea, we can say the correlation between the\n",
    "dependent variable and an explanatory variable can be magnified of yet\n",
    "another explanatory variable.\n",
    "\n",
    "To capture the limited effect of education on occupation, we can use an\n",
    "interaction term.\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image001.png\" >\n",
    "\n",
    "If you look at the wage equation, it becomes:\n",
    "\n",
    "$$wage = \\alpha + \\beta_{1}occupation + \\beta_{2}education + \\beta_{3}occupation*education + \\epsilon$$\n",
    "\n",
    "If $\\beta_{3}$ is positive, then it implies that an additional level of\n",
    "education yields a higher increase in the median value of a house for a\n",
    "high occupation level. In other words, there is an interaction effect\n",
    "between education and occupation.\n",
    "\n",
    "In this tutorial, we will try to see which variables can be a good\n",
    "candidate for interaction terms. We will test if adding this kind of\n",
    "information leads to better price prediction.\n",
    "\n",
    "## Summary statistics\n",
    "\n",
    "\n",
    "There are a few steps you can follow before proceeding to the model. As mentioned earlier, the model is a generalization of the data. The best practice is to understand the data and the make a prediction. If you do not know your data, you have slim chances to improve your model.\n",
    "\n",
    "As a first step, load the data as a pandas dataframe and create a training set and testing set.\n",
    "\n",
    "Tips:  For this tutorial, you need to have matplotlit and seaborn installed in Python. You can install Python package on the fly with Jupyter. You Should not do this\n",
    "\n",
    "```\n",
    "!conda install -- yes matplotlib\n",
    "````\n",
    "\n",
    "but \n",
    "\n",
    "```\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install matplotlib # Already installed\n",
    "\n",
    "!{sys.executable} -m pip install seaborn \n",
    "```\n",
    "\n",
    "Note that this step is not necessary if you have matplotlib and seaborn installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib is the library to create a graph in Python. Seaborn is a statistical visualization library built on top of matplotlib. It provides attractive and beautiful plots.\n",
    "\n",
    "The code below imports the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library `sklearn` includes the Boston dataset. You can call its API\n",
    "to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The featureâ€™s name are stored in the object `feature_names` in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names\n",
    "\n",
    "Output\n",
    "\n",
    "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = boston.feature_names\n",
    "df['PRICE'] = boston.target\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You convert the variable `CHAS` as a string variable and label it with\n",
    "`yes if CHAS = 1` and `no if CHAS = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CHAS'] = df['CHAS'].map({1:'yes', 0:'no'})\n",
    "\n",
    "df['CHAS'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pandas, it is straightforward to split the dataset. You randomly\n",
    "divide the dataset with 80 percent training set and 20 percent testing\n",
    "set. Pandas have a built-in function to split a data frame sample.\n",
    "\n",
    "The first parameter frac is a value from 0 to 1. You set it to 0.8 to\n",
    "select randomly 80 percent of the data frame.\n",
    "\n",
    "Random\\_state allows to have the same dataframe returned for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create train/test set\n",
    "df_train=df.sample(frac=0.8,random_state=200)\n",
    "df_test=df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the shape of the data. It should be:\n",
    "\n",
    "-   Train set: 506\\*0.8 = 405\n",
    "\n",
    "-   Test set: 506\\*0.2 = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is messy; it's often misbalanced and sprinkled with outlier values\n",
    "that throw off the analysis and machine learning training.\n",
    "\n",
    "**The first step to getting the dataset cleaned up is understanding\n",
    "where it needs cleaning.** Cleaning up a dataset can be tricky to do,\n",
    "especially in any generalizable manner\n",
    "\n",
    "Google Research team has developed a tool for this job called **Facets**\n",
    "that help to visualize the data and slice it in all sorts of manners.\n",
    "This is a good starting point to comprehend how the dataset is laid out.\n",
    "\n",
    "Facets allow you to find where the data does not quite look the way you\n",
    "are thinking.\n",
    "\n",
    "Except for their web app, Google makes it easy to embed the toolkit into\n",
    "a Jupyter notebook.\n",
    "\n",
    "There are two parts to Facets:\n",
    "\n",
    "-   Facets Overview\n",
    "\n",
    "-   Facets Deep Dive\n",
    "\n",
    "## Facets Overview\n",
    "\n",
    "\n",
    "Facets Overview gives an overview of the dataset. Facets Overview splits\n",
    "the columns of the data into rows of salient information showing\n",
    "\n",
    "1.  the percentage of missing observation\n",
    "\n",
    "2.  min and max values\n",
    "\n",
    "3.  statistics like the mean, median, and standard deviation.\n",
    "\n",
    "4.  It also adds a column that shows the percentage of values that are\n",
    "    zeroes, which is helpful when most of the values are zeroes.\n",
    "\n",
    "5.  It is possible to see these distributions on the test dataset as\n",
    "    well as the training set for each feature. It means you can\n",
    "    double-check that the test has a similar distribution to the\n",
    "    training dataset.\n",
    "\n",
    "This is at least the minimum to do before any machine learning task.\n",
    "With this tool, you do not miss this crucial step, and it highlights\n",
    "some abnormalities.\n",
    "\n",
    "## Facets Deep Dive\n",
    "\n",
    "\n",
    "Facets Deep Dive is a cool tool. It allowsto have some clarity on your\n",
    "dataset and zoom all the way in to see an individual piece of data. It\n",
    "means you can facet the data by row and column across any of the\n",
    "features of the dataset.\n",
    "\n",
    "We will use these two tools with the Boston dataset.\n",
    "\n",
    "**Note**: You cannot use Facets Overview and Facets Deep Dive at the\n",
    "same time. You need to clear the notebook first to change the tool.\n",
    "\n",
    "## Install Facet\n",
    "\n",
    "\n",
    "You can use the Facet web app for most of the analysis. In this\n",
    "tutorial, you will see how to use it within a Jupyter Notebook.\n",
    "\n",
    "First of all, you need to install `nbextensions`. It is done with this\n",
    "code. You copy and paste the following code in the terminal of your\n",
    "machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install jupyter_contrib_nbextensions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right after that, you need to clone the repositories in your computer.\n",
    "You have two choices:\n",
    "\n",
    "**Option 1)** Copy and paste this code in the terminal **(Recommended)**\n",
    "\n",
    "If you do not have Git installed on your machine, please go to this URL\n",
    "<https://git-scm.com/download/win> and follow the instruction. Once you\n",
    "are done, you can use the git command in the terminal for Mac User or\n",
    "Anaconda prompt for Windows user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "git clone https://github.com/PAIR-code/facets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2)** Go to <https://github.com/PAIR-code/facets> and download\n",
    "the repositories.\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image006.png\" >\n",
    "\n",
    "If you choose the first option, the file ends up in your download file.\n",
    "You can either let the file in download or drag it to another path.\n",
    "\n",
    "You can check where Facets is stored with this command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "echo `pwd`/`ls facets`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have located Facets, you need to install it in Jupyter\n",
    "Notebook. You need to set the working directory to the path where facets\n",
    "is located.\n",
    "\n",
    "Your present working directory and location of Facets zip should be\n",
    "same.\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image008.png\" >\n",
    "\n",
    "You need to point the working directory to Facet:\n",
    "\n",
    "cd facets\n",
    "\n",
    "To install Facets in Jupyter, you have two options. If you installed\n",
    "Jupyter with Conda for all the users, copy this code:\n",
    "\n",
    "can use `jupyter nbextension install facets-dist/`\n",
    "\n",
    "```\n",
    "jupyter nbextension install facets-dist/\n",
    "```\n",
    "Otherwise, use:\n",
    "```\n",
    "jupyter nbextension install facets-dist/ --user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, you are all set. Letâ€™s open Facet Overview.\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "Overview uses a Python script to compute the statistics. You need to\n",
    "import the script called generic\\_feature\\_statistics\\_generator to\n",
    "Jupyter. Don't worry; the script is located in the facets files.\n",
    "\n",
    "You need to locate its path. It is easily done. You open facets, open\n",
    "the file facets\\_overview and then python. Copy the path\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image010.png\" >\n",
    "\n",
    "After that, go back to Jupyter, and write the following code. Change the\n",
    "path `'/Users/Thomas/facets/facets_overview/python'` to your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the facets overview python code to the python path# Add t \n",
    "import sys\n",
    "sys.path.append('/Users/Thomas/facets/facets_overview/python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import the script with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n",
    "\n",
    "In windows, the same code becomes\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Admin\\Anaconda3\\facets-master\\facets_overview\\python\")\n",
    "\n",
    "from generic_feature_statistics_generator import GenericFeatureStatisticsGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the feature statistics, you need to use the function\n",
    "`GenericFeatureStatisticsGenerator(``)`, and you use the object\n",
    "`ProtoFromDataFrames`. You can pass the data frame in a dictionary. For\n",
    "instance, if we want to create a summary statistic for the train set, we\n",
    "can store the information in a dictionary and use it in the object\n",
    "\\`ProtoFromDataFrames\\`\\`\n",
    "\n",
    "-   `'name': 'train', 'table': df_train`\n",
    "\n",
    "`Name` is the name of the table displays, and you use the name of the\n",
    "table you want to compute the summary. In your example, the table\n",
    "containing the data is `df_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the feature statistics proto from the datasets and stringify it for use in facets overview\n",
    "import base64\n",
    "\n",
    "gfsg = GenericFeatureStatisticsGenerator()\n",
    "\n",
    "proto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': df_train},\n",
    "                                  {'name': 'test', 'table': df_test}])\n",
    "\n",
    "#proto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': df_train}])\n",
    "protostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you just copy and paste the code below. The code comes directly\n",
    "from GitHub. You should be able to see this:\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image012.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the facets overview visualization for this data# Displ \n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-dist/facets-jupyter.html\" >\n",
    "        <facets-overview id=\"elem\"></facets-overview>\n",
    "        <script>\n",
    "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "\n",
    "After you check the data and their distribution, you can plot a correlation matrix. The correlation matrix computes the Pearson coefficient. This coefficient is bonded between -1 and 1, with a positive value indicates a positive correlation and negative value a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You are interested to see which variables can be a good candidate for interaction terms.\n",
    "\n",
    "## Choose important feature and further check with Dive\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr('pearson')\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,annot=True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "png\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image014.png\" >\n",
    "\n",
    "From the matrix, you can see:\n",
    "\n",
    "-   `LSTAT`\n",
    "\n",
    "-   `RM`\n",
    "\n",
    "Are strongly correlated with `PRICE`. Another exciting feature is the\n",
    "strong positive correlation between `NOX` and `INDUS`, which means those\n",
    "two variables move in the same direction. Besides, there are also\n",
    "correlated with the `PRICE`. `DIS` is also highly correlated with `IND`\n",
    "and `NOX`.\n",
    "\n",
    "You have some first hint that `IND` and `NOX` can be good candidates for\n",
    "the interaction term and `DIS` might also be interesting to focus on.\n",
    "\n",
    "You can go a little bit deeper by plotting a pair grid. It will\n",
    "illustrate more in detail the correlation map you plotted before.\n",
    "\n",
    "The pair grid we are composed as follow:\n",
    "\n",
    "-   Upper part: Scatter plot with fitted line\n",
    "\n",
    "-   Diagonal: Kernel density plot\n",
    "\n",
    "-   Lower part: Multivariate kernel density plot\n",
    "\n",
    "You choose the focus on four independent variables. The choice\n",
    "corresponds to the variables with strong correlation with `PRICE`\n",
    "\n",
    "-   `INDUS`\n",
    "\n",
    "-   `NOX`\n",
    "\n",
    "-   `RM`\n",
    "\n",
    "-   `LSTAT`\n",
    "\n",
    "moreover, the `PRICE`.\n",
    "\n",
    "**Note** that the standard error is added by default to the scatter\n",
    "plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"PRICE\", \"INDUS\", \"NOX\", \"RM\", \"LSTAT\"]\n",
    "\n",
    "g = sns.PairGrid(df[attributes])\n",
    "g = g.map_upper(sns.regplot, color=\"g\")\n",
    "g = g.map_lower(sns.kdeplot,cmap=\"Reds\", shade=True, shade_lowest=False)\n",
    "g = g.map_diag(sns.kdeplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image016.png\" >\n",
    "\n",
    "Letâ€™s begin with the upper part:\n",
    "\n",
    "-   Price is negatively correlated with `INDUS`, `NOX`, and `LSTAT`;\n",
    "    positively correlated with `RM`.\n",
    "\n",
    "-   There is a slightly non-linearity with `LSTAT` and `PRICE`\n",
    "\n",
    "-   There is like a straight line when the price is equal to 50. From\n",
    "    the description of the dataset, `PRICE` has been truncated at the\n",
    "    value of 50\n",
    "\n",
    "Diagonal\n",
    "\n",
    "-   `NOX` seems to have two clusters, one around 0.5 and one around\n",
    "    0.85.\n",
    "\n",
    "\n",
    "\n",
    "To check more about it, you can look at the lower part. The Multivariate Kernel Density is interesting in a sense it colors where most of the points are. The difference with the scatter plot draws a probability density, even though there is no point in the dataset for a given coordinate. When the color is stronger, it indicates a high concentration of point around this area.\n",
    "\n",
    "If you check the multivariate density for INDUS and NOX, you can see the positive correlation and the two clusters. When the share of the industry is above 18, the nitric oxides concentration is above 0.6.\n",
    "\n",
    "You can think about adding an interaction between INDUS and NOX in the linear regression.\n",
    "\n",
    "Finally, you can use the second tools created by Google, Facets Deep Dive.\n",
    "The interface is divided up into four main sections. The central area in the center is a zoomable display of the data. On the top of the panel, there is the drop-down menu where you can change the arrangement of the data to controls faceting, positioning, and color. On the right, there is a detailed view of a specific row of data. It means you can click on any dot of data in the center visualization to see the detail about that particular data point.\n",
    "\n",
    "During the data visualization step, you are interested in looking for the pairwise correlation between the independent variable on the price of the house. However, it involves at least three variables, and 3D plots are complicated to work with.\n",
    "\n",
    "One way to tackle this problem is to create a categorical variable. That is, we can create a 2D plot a color the dot. You can split the variable PRICE into four categories, with each category is a quartile (i.e., 0.25, 0.5, 0.75). You call this new variable Q_PRICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check non linearity with important features\n",
    "df['Q_PRICE'] =  pd.qcut(df['PRICE'], 4, labels=[\"Lowest\", \"Low\", \"Upper\", \"upper_plus\"])\n",
    "## Show non linearity between RM and LSTAT\n",
    "ax = sns.lmplot(x=\"DIS\", y=\"INDUS\", hue=\"Q_PRICE\", data=df, fit_reg = False,\n",
    "               palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facets Deep Dive\n",
    "\n",
    "\n",
    "To open Deep Dive, you need to transform the data into a json format.\n",
    "Pandas as an object for that. You can use to\\_json after the Pandas\n",
    "dataset.\n",
    "\n",
    "The first line of code handle the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Q_PRICE'] =  pd.qcut(df['PRICE'], 4, labels=[\"Lowest\", \"Low\", \"Upper\", \"upper_plus\"])\n",
    "sprite_size = 32 if len(df.index)>50000 else 64\n",
    "jsonstr = df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below comes from Google GitHub. After you run the code, you\n",
    "should be able to see this:\n",
    "\n",
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image018.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display thde Dive visualization for this data\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Create Facets template  \n",
    "HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-dist/facets-jupyter.html\">\n",
    "        <facets-dive sprite-image-width=\"{sprite_size}\" sprite-image-height=\"{sprite_size}\" id=\"elem\" height=\"600\"></facets-dive>\n",
    "        <script>\n",
    "          document.querySelector(\"#elem\").data = {jsonstr};\n",
    "        </script>\"\"\"\n",
    "\n",
    "# Load the json dataset and the sprite_size into the template\n",
    "html = HTML_TEMPLATE.format(jsonstr=jsonstr, sprite_size=sprite_size)\n",
    "\n",
    "# Display the template\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are interested to see if there is a connection between the industry\n",
    "rate, oxide concentration, distance to the job center and the price of\n",
    "the house.\n",
    "\n",
    "For that, you first split the data by industry range and color with the\n",
    "price quartile:\n",
    "\n",
    "-   Select faceting X and choose `INDUS`.\n",
    "\n",
    "-   Select Display and choose `DIS`. It will color the dots with the\n",
    "    quartile of the house price\n",
    "\n",
    "here, darker colors mean the distance to the first job center is far.\n",
    "\n",
    "So far, it shows again what you know, lower industry rate, higher price.\n",
    "Now you can look at the breakdown by `INDUX`, by `NOX`.\n",
    "\n",
    "-   Select faceting Y and choose `NOX`.\n",
    "\n",
    "Now you can see the house far from the first job center have the lowest\n",
    "industry share and therefore the lowest oxide concentration. If you\n",
    "choose to display the type with `Q_PRICE` and zoom the lower-left\n",
    "corner, you can see what type of price it is.\n",
    "\n",
    "You have another hint that the interaction between `IND`, `NOX`, and\n",
    "`DIS` can be good candidates to improve the model.\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "\n",
    "In this section, you will estimate the linear classifier with TensorFlow\n",
    "estimators API. You will proceed as follow:\n",
    "\n",
    "-   Prepare the data\n",
    "\n",
    "-   Estimate a benchmark model: No interaction\n",
    "\n",
    "-   Estimate a model with interaction\n",
    "\n",
    "Remember, the goal of machine learning is the minimize the error. In\n",
    "this case, the model with the lowest mean square error will win. The\n",
    "TensorFlow estimator automatically computes this metric.\n",
    "\n",
    "### Preparation data\n",
    "\n",
    "In most of the case, you need to transform your data. That is why Facets Overview is fascinating. From the summary statistic, you saw there are outliers. Those values affect the estimates because they do not look like the population you are analyzing. Outliers usually biased the results. For instance, a positive outlier tends to overestimate the coefficient.\n",
    "\n",
    "A good solution to tackle this problem is to standardize the variable. Standardization means a standard deviation of one and means of zero. The process of standardization involves two steps. First of all, it subtracts the mean value of the variable. Secondly, it divides by the variance so that the distribution has a unit variance\n",
    "\n",
    "The library sklearn is helpful to standardize variables. You can use the module preprocessing with the object scale for this purpose.\n",
    "\n",
    "You can use the function below to scale a dataset. Note that you donâ€™t scale the label column and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def standardize_data(df): \n",
    "    X_scaled = preprocessing.scale(df[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "       'TAX', 'PTRATIO', 'B', 'LSTAT']])\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "       'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "    df_scale = pd.concat([X_scaled_df,\n",
    "                       df['CHAS'],\n",
    "                       df['PRICE']],axis=1, join='inner')\n",
    "    return df_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function to construct the scaled train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scale = standardize_data(df_train)\n",
    "df_test_scale = standardize_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic regression:Benchmark\n",
    "\n",
    "First of all, you train and test a model without interaction. The\n",
    "purpose is to see the performance metric of the model.\n",
    "\n",
    "The way to train the model is exactly as the tutorial on **High-level\n",
    "API**. You will use the TensorFlow estimator `LinearRegressor`.\n",
    "\n",
    "As a reminder, you need to choose:\n",
    "\n",
    "-   the features to put in the model\n",
    "\n",
    "-   transform the features\n",
    "\n",
    "-   construct the linear regressor\n",
    "\n",
    "-   construct the `input_fn` function\n",
    "\n",
    "-   train the model\n",
    "\n",
    "-   test the model\n",
    "\n",
    "You use all the variables in the dataset to train the model. In total,\n",
    "there are elevel continuous variables and one categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add features to the bucket: \n",
    "### Define continuous list\n",
    "CONTI_FEATURES  = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "CATE_FEATURES = ['CHAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You convert the features into a numeric column or categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [tf.feature_column.numeric_column(k) for k in CONTI_FEATURES]\n",
    "#categorical_features = tf.feature_column.categorical_column_with_hash_bucket(CATE_FEATURES, hash_bucket_size=1000)\n",
    "categorical_features = [tf.feature_column.categorical_column_with_vocabulary_list('CHAS', ['yes','no'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create the model with the `linearRegressor`. You store the model in\n",
    "the folder `train_Boston`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.LinearRegressor(\n",
    "    model_dir=\"train_Boston\", \n",
    "    feature_columns=categorical_features + continuous_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in the train or test data is converted into a Tensor with\n",
    "the the function `get_input_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT', 'CHAS']\n",
    "LABEL= 'PRICE'\n",
    "def get_input_fn(data_set, num_epochs=None, n_batch = 128, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "       x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "       y = pd.Series(data_set[LABEL].values),\n",
    "       batch_size=n_batch,   \n",
    "       num_epochs=num_epochs,\n",
    "       shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You estimate the model on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(input_fn=get_input_fn(df_train_scale, \n",
    "                                      num_epochs=None,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, you estimate the performances of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(input_fn=get_input_fn(df_test_scale, \n",
    "                                      num_epochs=1,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of the model is 1650. This is the metric to beat in the next\n",
    "section\n",
    "\n",
    "## Improve the model: Interaction term\n",
    "\n",
    "\n",
    "During the first part of the tutorial, you saw an interesting relationship between the variables. The different visualization techniques revealed that INDUS and NOS are linked together and turns to magnify the effect on the price. Not only the interaction between INDUS and NOS affects the price but also this effect is stronger when it interacts with DIS.\n",
    "\n",
    "It is time to generalize this idea and see if you can improve the model prediction.\n",
    "\n",
    "You need to add two new columns to each dataset set: train + test. For that, you create one function to compute the interaction term and another one to compute the triple interaction term. Each function produces a single column. After the new variables are created, you can concatenate them to the training dataset and test dataset.\n",
    "\n",
    "First of all, you need to create a new variable for the interaction between INDUS and NOX.\n",
    "\n",
    "The function below returns two dataframes, train and test, with the interaction between var_1 and var_2, in your case INDUS and NOX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_term(var_1, var_2, name):\n",
    "    t_train = df_train_scale[var_1]*df_train_scale[var_2]\n",
    "    train = t_train.rename(name)\n",
    "    t_test = df_test_scale[var_1]*df_test_scale[var_2]\n",
    "    test = t_test.rename(name)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You store the two new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interation_ind_ns_train, interation_ind_ns_test= interaction_term('INDUS', 'NOX', 'INDUS_NOS')\n",
    "\n",
    "interation_ind_ns_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, you create a second function to compute the triple interaction\n",
    "term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_interaction_term(var_1, var_2,var_3, name):\n",
    "    t_train = df_train_scale[var_1]*df_train_scale[var_2]*df_train_scale[var_3]\n",
    "    train = t_train.rename(name)\n",
    "    t_test = df_test_scale[var_1]*df_test_scale[var_2]*df_test_scale[var_3]\n",
    "    test = t_test.rename(name)\n",
    "    return train, test\n",
    "\n",
    "interation_ind_ns_dis_train, interation_ind_ns_dis_test= triple_interaction_term('INDUS', 'NOX', 'DIS','INDUS_NOS_DIS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have all columns needed, you can add them to train and test\n",
    "dataset. You name these two new dataframe:\n",
    "\n",
    "-   df\\_train\\_new\n",
    "\n",
    "-   df\\_test\\_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new = pd.concat([df_train_scale,\n",
    "                          interation_ind_ns_train,\n",
    "                          interation_ind_ns_dis_train],\n",
    "                         axis=1, join='inner')\n",
    "\n",
    "df_test_new = pd.concat([df_test_scale,\n",
    "                         interation_ind_ns_test,\n",
    "                         interation_ind_ns_dis_test],\n",
    "                         axis=1, join='inner')\n",
    "\n",
    "df_train_new.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tensorflow/16_Linear_regression_case_study_v5_files/image011.png\" >\n",
    "\n",
    "That is it; you can estimate the new model with the interaction terms\n",
    "and see how is the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTI_FEATURES_NEW  = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT',\n",
    "                       'INDUS_NOS', 'INDUS_NOS_DIS']\n",
    "### Define categorical list\n",
    "continuous_features_new = [tf.feature_column.numeric_column(k) for k in CONTI_FEATURES_NEW]\n",
    "\n",
    "model = tf.estimator.LinearRegressor(\n",
    "    model_dir=\"train_Boston_1\", \n",
    "    feature_columns= categorical_features + continuous_features_new)\n",
    "\n",
    "FEATURES = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD','TAX', 'PTRATIO', 'B', 'LSTAT',\n",
    "            'INDUS_NOS', 'INDUS_NOS_DIS','CHAS']\n",
    "LABEL= 'PRICE'\n",
    "def get_input_fn(data_set, num_epochs=None, n_batch = 128, shuffle=True):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "       x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "       y = pd.Series(data_set[LABEL].values),\n",
    "       batch_size=n_batch,   \n",
    "       num_epochs=num_epochs,\n",
    "       shuffle=shuffle)\n",
    "\n",
    "model.train(input_fn=get_input_fn(df_train_new, \n",
    "                                      num_epochs=None,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(input_fn=get_input_fn(df_test_new, \n",
    "                                      num_epochs=1,\n",
    "                                      n_batch = 128,\n",
    "                                      shuffle=False),\n",
    "                                      steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new loss is 1515. Just by adding two new variables, you were able to\n",
    "decrease the loss. It means you can make a better prediction than with\n",
    "the benchmark model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
