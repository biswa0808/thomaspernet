{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Introduction to Scikit learn\n",
    "author: Thomas\n",
    "date: '2018-08-29'\n",
    "slug: scikit\n",
    "categories: []\n",
    "tags:\n",
    "  - tf-install\n",
    "header:\n",
    "  caption: ''\n",
    "  image: ''\n",
    "--- \n",
    "\n",
    "# What is Scikit-learn?\n",
    "\n",
    "\n",
    "Scikit-learn is an open source Python library for machine learning. The\n",
    "library supports state-of-the-art algorithms such as KNN, XGBoost,\n",
    "random forest, SVM among others. It is built on top of Numpy.\n",
    "Scikit-learn is widely used in kaggle competition as well as prominent\n",
    "tech companies. Scikit-learn helps in preprocessing, dimensionality\n",
    "reduction(parameter selection), classification, regression, clustering,\n",
    "and model selection.\n",
    "\n",
    "Scikit-learn has the best documentation of all opensource libraries. It\n",
    "provides you an interactive chart at\n",
    "<http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html>.\n",
    "\n",
    "Scikit-learn is not very difficult to use and provides excellent\n",
    "results. However, scikit learn does not support parallel computations.\n",
    "It is possible to run a deep learning algorithm with it but is not an\n",
    "optimal solution, especially if you know how to use TensorFlow.\n",
    "\n",
    "In this tutorial, you will learn.\n",
    "\n",
    "## Download and Install scikit-learn\n",
    "\n",
    "\n",
    "**Option 1:** AWS\n",
    "\n",
    "scikit-learn can be used over AWS. Please refer to the part on AWS. The docker\n",
    "image has scikit-learn preinstalled.\n",
    "\n",
    "To use developer version use the command in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install git+git://github.com/scikit-learn/scikit-learn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:** Mac or Windows using Anaconda\n",
    "\n",
    "Recently, the developers of scikit have released a development version\n",
    "that tackles common problem faced with the current version. We found it\n",
    "more convenient to use the developer version instead of the current\n",
    "version.\n",
    "\n",
    "If you installed scikit-learn with the conda environment, please follow\n",
    "the step to update to version 0.20\n",
    "\n",
    "**Step 1)** Activate tensorflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source activate hello-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2)** Remove scikit lean using the conda command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda remove scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3)** Install scikit learn developer version along with necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c anaconda git\n",
    "#pip install Cython\n",
    "#pip install h5py\n",
    "#pip install git+git://github.com/scikit-learn/scikit-learn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Windows used will need to install Microsoft Visual C++ 14. You can get it from here\n",
    "\n",
    "This part is divided into two parts:\n",
    "\n",
    "1.  Machine learning with scikit-learn\n",
    "\n",
    "2.  How to trust your model with LIME\n",
    "\n",
    "The first part details how to build a pipeline, create a model and tune\n",
    "the hyperparameters while the second part provides state-of-the-art in\n",
    "term of model selection.\n",
    "\n",
    "## Step 1) Import the data\n",
    "\n",
    "During this tutorial, you will be using the `adult` dataset. For a\n",
    "background in this dataset refer If you are interested to know more\n",
    "about the descriptive statistics, please use Dive and Overview tools.\n",
    "Refer this tutorial learn more about Dive and Overview\n",
    "\n",
    "You import the dataset with Pandas. Note that you need to convert the\n",
    "type of the continuous variables in float format.\n",
    "\n",
    "This dataset includes eights categorical variables:\n",
    "\n",
    "The categorical variables are listed in `CATE_FEATURES`\n",
    "\n",
    "-   workclass\n",
    "\n",
    "-   education\n",
    "\n",
    "-   marital\n",
    "\n",
    "-   occupation\n",
    "\n",
    "-   relationship\n",
    "\n",
    "-   race\n",
    "\n",
    "-   sex\n",
    "\n",
    "-   native_country\n",
    "\n",
    "moreover, six continuous variables:\n",
    "\n",
    "The continuous variables are listed in `CONTI_FEATURES`\n",
    "\n",
    "-   age\n",
    "\n",
    "-   fnlwgt\n",
    "\n",
    "-   education_num\n",
    "\n",
    "-   capital_gain\n",
    "\n",
    "-   capital_loss\n",
    "\n",
    "-   hours_week\n",
    "\n",
    "Note that we fill the list by hand so that you have a better idea of\n",
    "what columns we are using. A faster way to construct a list of\n",
    "categorical or continuous is to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "## List Categorical\n",
    "CATE_FEATURES = df_train.iloc[:,:-1].select_dtypes('object').columns\n",
    "print(CATE_FEATURES)\n",
    "## List continuous\n",
    "CONTI_FEATURES =  df_train._get_numeric_data()\n",
    "print(CONTI_FEATURES)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to import the data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "         hours_week  \n",
       "count  32561.000000  \n",
       "mean      40.437456  \n",
       "std       12.347429  \n",
       "min        1.000000  \n",
       "25%       40.000000  \n",
       "50%       40.000000  \n",
       "75%       45.000000  \n",
       "max       99.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "import pandas as pd\n",
    "\n",
    "## Define path data\n",
    "COLUMNS = ['age','workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "           'hours_week', 'native_country', 'label']\n",
    "### Define continuous list\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "### Define categorical list\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "## Prepare the data\n",
    "features = ['age','workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "           'hours_week', 'native_country']\n",
    "\n",
    "PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "df_train = pd.read_csv(PATH, skipinitialspace=True, names = COLUMNS, index_col=False)\n",
    "df_train[CONTI_FEATURES] =df_train[CONTI_FEATURES].astype('float64')\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the count of unique values of the `native_country`\n",
    "features. You can see that only one household comes from\n",
    "`Holand-Netherlands`. This household won't bring us any information, but\n",
    "will through an error during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United-States                 29170\n",
       "Mexico                          643\n",
       "?                               583\n",
       "Philippines                     198\n",
       "Germany                         137\n",
       "Canada                          121\n",
       "Puerto-Rico                     114\n",
       "El-Salvador                     106\n",
       "India                           100\n",
       "Cuba                             95\n",
       "England                          90\n",
       "Jamaica                          81\n",
       "South                            80\n",
       "China                            75\n",
       "Italy                            73\n",
       "Dominican-Republic               70\n",
       "Vietnam                          67\n",
       "Guatemala                        64\n",
       "Japan                            62\n",
       "Poland                           60\n",
       "Columbia                         59\n",
       "Taiwan                           51\n",
       "Haiti                            44\n",
       "Iran                             43\n",
       "Portugal                         37\n",
       "Nicaragua                        34\n",
       "Peru                             31\n",
       "Greece                           29\n",
       "France                           29\n",
       "Ecuador                          28\n",
       "Ireland                          24\n",
       "Hong                             20\n",
       "Trinadad&Tobago                  19\n",
       "Cambodia                         19\n",
       "Thailand                         18\n",
       "Laos                             18\n",
       "Yugoslavia                       16\n",
       "Outlying-US(Guam-USVI-etc)       14\n",
       "Hungary                          13\n",
       "Honduras                         13\n",
       "Scotland                         12\n",
       "Holand-Netherlands                1\n",
       "Name: native_country, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.native_country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can exclude this uninformative row from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Netherland, because only one row\n",
    "df_train = df_train[df_train.native_country != \"Holand-Netherlands\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you store the position of the continuous features in a list. You\n",
    "will need it in the next step to build the pipeline.\n",
    "\n",
    "The code below will loop over all columns names in `CONTI_FEATURES` and\n",
    "get its location (i.e., its number) and then append it to a list called\n",
    "`conti_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 10, 4, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "## Get the column index of the categorical features\n",
    "conti_features = []\n",
    "for i in CONTI_FEATURES:\n",
    "    position = df_train.columns.get_loc(i)\n",
    "    conti_features.append(position)\n",
    "print(conti_features)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the same job as above but for the categorical\n",
    "variable. The code below repeats what you have done previously, except\n",
    "with the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 7, 8, 9, 13]\n"
     ]
    }
   ],
   "source": [
    "## Get the column index of the categorical features\n",
    "categorical_features = []\n",
    "for i in CATE_FEATURES:\n",
    "    position = df_train.columns.get_loc(i)\n",
    "    categorical_features.append(position)\n",
    "print(categorical_features)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look at the dataset. Note that, each categorical feature\n",
    "is a string. You cannot feed a model with a string value. You need to\n",
    "transform the dataset using a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age         workclass    fnlwgt  education  education_num  \\\n",
       "0  39.0         State-gov   77516.0  Bachelors           13.0   \n",
       "1  50.0  Self-emp-not-inc   83311.0  Bachelors           13.0   \n",
       "2  38.0           Private  215646.0    HS-grad            9.0   \n",
       "3  53.0           Private  234721.0       11th            7.0   \n",
       "4  28.0           Private  338409.0  Bachelors           13.0   \n",
       "\n",
       "              marital         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_week native_country  label  \n",
       "0        2174.0           0.0        40.0  United-States  <=50K  \n",
       "1           0.0           0.0        13.0  United-States  <=50K  \n",
       "2           0.0           0.0        40.0  United-States  <=50K  \n",
       "3           0.0           0.0        40.0  United-States  <=50K  \n",
       "4           0.0           0.0        40.0           Cuba  <=50K  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you need to create one column for each group in the feature.\n",
    "First, you can run the code below to compute the total amount of columns\n",
    "needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass          9\n",
      "education         16\n",
      "marital            7\n",
      "occupation        15\n",
      "relationship       6\n",
      "race               5\n",
      "sex                2\n",
      "native_country    41\n",
      "dtype: int64 There are 101 groups in the whole dataset\n"
     ]
    }
   ],
   "source": [
    "print(df_train[CATE_FEATURES].nunique(),\n",
    "      'There are',sum(df_train[CATE_FEATURES].nunique()), 'groups in the whole dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole dataset contains 101 groups as shown above. For instance, the\n",
    "features `of ``workclass` have nine groups. You can visualize the name\n",
    "of the groups with the following codes\n",
    "\n",
    "`unique()` returns the unique values of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['State-gov' 'Self-emp-not-inc' 'Private' 'Federal-gov' 'Local-gov' '?'\n",
      " 'Self-emp-inc' 'Without-pay' 'Never-worked']\n",
      "['Bachelors' 'HS-grad' '11th' 'Masters' '9th' 'Some-college' 'Assoc-acdm'\n",
      " 'Assoc-voc' '7th-8th' 'Doctorate' 'Prof-school' '5th-6th' '10th'\n",
      " '1st-4th' 'Preschool' '12th']\n",
      "['Never-married' 'Married-civ-spouse' 'Divorced' 'Married-spouse-absent'\n",
      " 'Separated' 'Married-AF-spouse' 'Widowed']\n",
      "['Adm-clerical' 'Exec-managerial' 'Handlers-cleaners' 'Prof-specialty'\n",
      " 'Other-service' 'Sales' 'Craft-repair' 'Transport-moving'\n",
      " 'Farming-fishing' 'Machine-op-inspct' 'Tech-support' '?'\n",
      " 'Protective-serv' 'Armed-Forces' 'Priv-house-serv']\n",
      "['Not-in-family' 'Husband' 'Wife' 'Own-child' 'Unmarried' 'Other-relative']\n",
      "['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other']\n",
      "['Male' 'Female']\n",
      "['United-States' 'Cuba' 'Jamaica' 'India' '?' 'Mexico' 'South'\n",
      " 'Puerto-Rico' 'Honduras' 'England' 'Canada' 'Germany' 'Iran'\n",
      " 'Philippines' 'Italy' 'Poland' 'Columbia' 'Cambodia' 'Thailand' 'Ecuador'\n",
      " 'Laos' 'Taiwan' 'Haiti' 'Portugal' 'Dominican-Republic' 'El-Salvador'\n",
      " 'France' 'Guatemala' 'China' 'Japan' 'Yugoslavia' 'Peru'\n",
      " 'Outlying-US(Guam-USVI-etc)' 'Scotland' 'Trinadad&Tobago' 'Greece'\n",
      " 'Nicaragua' 'Vietnam' 'Hong' 'Ireland' 'Hungary']\n"
     ]
    }
   ],
   "source": [
    "for i in CATE_FEATURES:\n",
    "    print(df_train[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the training dataset will contain 101 + 7 columns. The last\n",
    "seven columns are the continuous features.\n",
    "\n",
    "Scikit-learn can take care of the conversion. It is done in two steps:\n",
    "\n",
    "-   First, you need to convert the string to ID. For instance,\n",
    "    `State-gov` will have the ID 1, `Self-emp-not-inc` ID 2 and so on.\n",
    "    The function `LabelEncoder` does this for you\n",
    "\n",
    "-   Transpose each ID into a new column. As mentioned before, the\n",
    "    dataset has 101 group's ID. Therefore there will be 101 columns\n",
    "    capturing all categoricals features' groups. Scikit-learn has a\n",
    "    function called `OneHotEncoder` that performs this operation\n",
    "\n",
    "## Step 2) Create the train/test set\n",
    "\n",
    "\n",
    "Now that the dataset is ready, we can split it 80/20. 80 percent for the\n",
    "training set and 20 percent for the test set.\n",
    "\n",
    "You can use `train_test_split`. The first argument is the dataframe is\n",
    "the features and the second argument is the label dataframe. You can\n",
    "specify the size of the test set with `test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26048, 14) (6512, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[features],\n",
    "                                                    df_train.label,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.head(5)\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Build the pipeline\n",
    "\n",
    "\n",
    "The pipeline makes it easier to feed the model with consistent data. The\n",
    "idea behind is to put the raw data into a 'pipeline' to perform\n",
    "operations. For instance, with the current dataset, you need to\n",
    "standardize the continuous variables and convert the categorical data.\n",
    "Note that you can perform any operation inside the pipeline. For\n",
    "instance, if you have 'NA's' in the dataset, you can replace them by the\n",
    "mean or median. You can also create new variables.\n",
    "\n",
    "You have the choice; hard code the two processes or create a pipeline.\n",
    "The first choice can lead to data leakage and create inconsistencies\n",
    "over time. A better option is to use the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will perform two operations before feeding the logistic\n",
    "classifier:\n",
    "\n",
    "1.  Standardize the variable: `StandardScaler()``\n",
    "\n",
    "2.  Convert the categorical features: `OneHotEncoder(sparse=False)`\n",
    "\n",
    "You can perform the two steps using the `make_column_transformer`. This\n",
    "function is not available in the current version of scikit-learn (0.19).\n",
    "It is not possible with the current version to perform the label encoder\n",
    "and one hot encoder in the pipeline. It is one reason we decided to use\n",
    "the developer version.\n",
    "\n",
    "`make_column_transformer` is easy to use. You need to define which\n",
    "columns to apply the transformation and what transformation to operate.\n",
    "For instance, to standardize the continuous feature, you can do:\n",
    "\n",
    "-   `conti_features, StandardScaler()` inside `make_column_transformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `conti_features`: list with the continuous variable\n",
    "\n",
    "-   `StandardScaler`: standardize the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `OneHotEncoder` inside `make_column_transformer`\n",
    "automatically encodes the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (conti_features, StandardScaler()),\n",
    "    ### Need to be numeric not string to specify columns name \n",
    "    (categorical_features, OneHotEncoder(sparse=False))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test if the pipeline works with `fit_transform`. The dataset\n",
    "should have the following shape: 26048, 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26048, 107)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.fit_transform(X_train).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data transformer is ready to use. You can create the pipeline with\n",
    "`make_pipeline`. Once the data are transformed, you can feed the\n",
    "logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    preprocess,\n",
    "    LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model with scikit-learn is trivial. You need to use the\n",
    "object `fit` preceded by the pipeline, i.e., `model`. You can print the\n",
    "accuracy with the `score` object from the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.850891\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "print(\"logistic regression score: %f\" % model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can predict the classes with `predict_proba`. It returns\n",
    "the probability for each class. Note that it sums to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83576282, 0.16423718],\n",
       "       [0.94582346, 0.05417654],\n",
       "       [0.64758444, 0.35241556],\n",
       "       ...,\n",
       "       [0.99639221, 0.00360779],\n",
       "       [0.02072329, 0.97927671],\n",
       "       [0.56782129, 0.43217871]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Using our pipeline in a grid search\n",
    "\n",
    "\n",
    "Tune the hyperparameter (variables that determine network structure like\n",
    "hidden units) can be tedious and exhausting. One way to evaluate the\n",
    "model could be to change the size of the training set and evaluate the\n",
    "performances. You can repeat this method ten times to see the score\n",
    "metrics. However, it is too much work.\n",
    "\n",
    "Instead, scikit-learn provides a function to carry out parameter tuning\n",
    "and cross-validation.\n",
    "\n",
    "**Cross-validation**\n",
    "\n",
    "Cross-Validation means during the training, the training set is slip `n`\n",
    "number of times in folds and then evaluates the model `n` time. For\n",
    "instance, if `cv` is set to 10, the training set is trained and\n",
    "evaluates ten times. At each round, the classifier chooses randomly nine\n",
    "fold to train the model, and the 10th fold is meant for evaluation.\n",
    "\n",
    "**Grid search** Each classifier has hyperparameters to tune. You can try\n",
    "different values, or you can set a parameter grid. If you go to the\n",
    "scikit-learn official website, you can see the logistic classifier has\n",
    "different parameters to tune. To make the training faster, you choose to\n",
    "tune the `C` parameter. It controls for the regularization parameter. It\n",
    "should be positive. A small value gives more weight to the regularizer.\n",
    "\n",
    "You can use the object `GridSearchCV`. You need to create a dictionary\n",
    "containing the hyperparameters to tune.\n",
    "\n",
    "You list the hyperparameters followed by the values you want to try. For\n",
    "instance, to tune the `C` parameter, you use:\n",
    "\n",
    "-   `'logisticregression__C': [0.1, 1.0, 1.0]`: The parameter is\n",
    "    preceded by the name, in lower case, of the classifier and two\n",
    "    underscores.\n",
    "\n",
    "The model will try four different values: 0.001, 0.01, 0.1 and 1.\n",
    "\n",
    "You train the model using 10 folds: `cv=10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Construct the parameter grid\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01,0.1, 1.0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the model using `GridSearchCV` with the parameter gri and\n",
    "cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('columntransformer', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True), [0, 2, 10, 4, 11, 12]), ('onehotencoder', OneHotEncoder(categoric...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'logisticregression__C': [0.001, 0.01, 0.1, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "grid_clf = GridSearchCV(model,\n",
    "                        param_grid,\n",
    "                        cv=10,\n",
    "                        iid=False)\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the best parameters, you use `best_params_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trained the model with four differents regularization values, the\n",
    "optimal parameter is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.850891\n"
     ]
    }
   ],
   "source": [
    "print(\"best logistic regression from grid search: %f\" % grid_clf.best_estimator_.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the predicted probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83576282, 0.16423718],\n",
       "       [0.94582346, 0.05417654],\n",
       "       [0.64758444, 0.35241556],\n",
       "       ...,\n",
       "       [0.99639221, 0.00360779],\n",
       "       [0.02072329, 0.97927671],\n",
       "       [0.56782129, 0.43217871]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_estimator_.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost\n",
    "\n",
    "\n",
    "Let's try to train one of the best classifiers on the market. XGBoost is\n",
    "an improvement over the random forest. The theoretical background of the\n",
    "classifier out of the scope of this tutorial. Keep in mind that, XGBoost\n",
    "has won lots of kaggle competitions. With an average dataset size, it\n",
    "can perform as good as a deep learning algorithm or even better.\n",
    "\n",
    "The classifier is challenging to train because it has a high number of\n",
    "parameters to tune. You can, of course, use `GridSearchCV` to choose the\n",
    "parameter for you.\n",
    "\n",
    "Instead, let's see how to use a better way to find the optimal\n",
    "parameters. `GridSearchCV` can be tedious and very long to train if you\n",
    "pass many values. The search space grows along with the number of\n",
    "parameters. A preferable solution is to use `RandomizedSearchCV`. This\n",
    "method consists of choosing the values of each hyperparameter after each\n",
    "iteration randomly. For instance, if the classifier is trained over 1000\n",
    "iterations, then 1000 combinations are evaluated. It works more or less\n",
    "like. `GridSearchCV`\n",
    "\n",
    "You need to import `xgboost`. If the library is not installed, please\n",
    "use `pip3 install xgboost` or \n",
    "\n",
    "\n",
    "```\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "```\n",
    "\n",
    "In Jupyter environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/1e/6d13dacd1d5ea3273210162292e818f82629328ce51cdb7eb633f03e0b52/xgboost-0.80.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    ++ pwd\n",
      "    + oldpath=/private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost\n",
      "    + cd ./xgboost/\n",
      "    + echo darwin18\n",
      "    + grep -q darwin\n",
      "    + LIB_XGBOOST=libxgboost.dylib\n",
      "    + which g++-5\n",
      "    + which g++-7\n",
      "    /usr/local/bin/g++-7\n",
      "    + export CC=gcc-7\n",
      "    + CC=gcc-7\n",
      "    + export CXX=g++-7\n",
      "    + CXX=g++-7\n",
      "    + make clean\n",
      "    Makefile:31: MAKE [/Library/Developer/CommandLineTools/usr/bin/make] - checked OK\n",
      "    rm -f -rf build build_plugin lib bin *~ */*~ */*/*~ */*/*/*~ */*.o */*/*.o */*/*/*.o #xgboost\n",
      "    rm -f -rf build_tests *.gcov tests/cpp/xgboost_test\n",
      "    if [ -d \"R-package/src\" ]; then \\\n",
      "    \t\tcd R-package/src; \\\n",
      "    \t\trm -f -rf rabit src include dmlc-core amalgamation *.so *.dll; \\\n",
      "    \t\tcd /private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/xgboost; \\\n",
      "    \tfi\n",
      "    + make lib/libxgboost.dylib -j4\n",
      "    Makefile:31: MAKE [/Library/Developer/CommandLineTools/usr/bin/make] - checked OK\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp -MM -MT build/logging.o src/logging.cc >build/logging.d\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp -MM -MT build/learner.o src/learner.cc >build/learner.d\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp -MM -MT build/c_api/c_api.o src/c_api/c_api.cc >build/c_api/c_api.d\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp -MM -MT build/c_api/c_api_error.o src/c_api/c_api_error.cc >build/c_api/c_api_error.d\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp src/c_api/c_api_error.cc -o build/c_api/c_api_error.o\n",
      "    In file included from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/bits/postypes.h:40:0,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/bits/char_traits.h:40,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/string:40,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/stdexcept:39,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/array:39,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/tuple:39,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/mutex:38,\n",
      "                     from dmlc-core/include/dmlc/thread_local.h:9,\n",
      "                     from src/c_api/c_api_error.cc:6:\n",
      "    /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/cwchar:44:10: fatal error: wchar.h: No such file or directory\n",
      "     #include <wchar.h>\n",
      "              ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/c_api/c_api_error.o] Error 1\n",
      "    make: *** Waiting for unfinished jobs....\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp src/logging.cc -o build/logging.o\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp src/c_api/c_api.cc -o build/c_api/c_api.o\n",
      "    In file included from dmlc-core/include/dmlc/logging.h:10:0,\n",
      "                     from include/xgboost/logging.h:11,\n",
      "                     from src/logging.cc:7:\n",
      "    /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/cstdio:42:10: fatal error: stdio.h: No such file or directory\n",
      "     #include <stdio.h>\n",
      "              ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/logging.o] Error 1\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -fopenmp src/learner.cc -o build/learner.o\n",
      "    In file included from include/xgboost/data.h:10:0,\n",
      "                     from src/c_api/c_api.cc:3:\n",
      "    dmlc-core/include/dmlc/base.h:203:10: fatal error: sys/types.h: No such file or directory\n",
      "     #include <sys/types.h>\n",
      "              ^~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/c_api/c_api.o] Error 1\n",
      "    In file included from dmlc-core/include/dmlc/io.h:8:0,\n",
      "                     from src/learner.cc:7:\n",
      "    /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/cstdio:42:10: fatal error: stdio.h: No such file or directory\n",
      "     #include <stdio.h>\n",
      "              ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/learner.o] Error 1\n",
      "    + echo -----------------------------\n",
      "    -----------------------------\n",
      "    + echo 'Building multi-thread xgboost failed'\n",
      "    Building multi-thread xgboost failed\n",
      "    + echo 'Start to build single-thread xgboost'\n",
      "    Start to build single-thread xgboost\n",
      "    + make clean\n",
      "    Makefile:31: MAKE [/Library/Developer/CommandLineTools/usr/bin/make] - checked OK\n",
      "    rm -f -rf build build_plugin lib bin *~ */*~ */*/*~ */*/*/*~ */*.o */*/*.o */*/*/*.o #xgboost\n",
      "    rm -f -rf build_tests *.gcov tests/cpp/xgboost_test\n",
      "    if [ -d \"R-package/src\" ]; then \\\n",
      "    \t\tcd R-package/src; \\\n",
      "    \t\trm -f -rf rabit src include dmlc-core amalgamation *.so *.dll; \\\n",
      "    \t\tcd /private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/xgboost; \\\n",
      "    \tfi\n",
      "    + make lib/libxgboost.dylib -j4 USE_OPENMP=0\n",
      "    Makefile:31: MAKE [/Library/Developer/CommandLineTools/usr/bin/make] - checked OK\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/logging.o src/logging.cc >build/logging.d\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/learner.o src/learner.cc >build/learner.d\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/c_api/c_api.o src/c_api/c_api.cc >build/c_api/c_api.d\n",
      "    g++-7 -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/c_api/c_api_error.o src/c_api/c_api_error.cc >build/c_api/c_api_error.d\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP src/c_api/c_api_error.cc -o build/c_api/c_api_error.o\n",
      "    In file included from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/bits/postypes.h:40:0,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/bits/char_traits.h:40,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/string:40,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/stdexcept:39,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/array:39,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/tuple:39,\n",
      "                     from /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/mutex:38,\n",
      "                     from dmlc-core/include/dmlc/thread_local.h:9,\n",
      "                     from src/c_api/c_api_error.cc:6:\n",
      "    /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/cwchar:44:10: fatal error: wchar.h: No such file or directory\n",
      "     #include <wchar.h>\n",
      "              ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/c_api/c_api_error.o] Error 1\n",
      "    make: *** Waiting for unfinished jobs....\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP src/logging.cc -o build/logging.o\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP src/c_api/c_api.cc -o build/c_api/c_api.o\n",
      "    In file included from dmlc-core/include/dmlc/logging.h:10:0,\n",
      "                     from include/xgboost/logging.h:11,\n",
      "                     from src/logging.cc:7:\n",
      "    /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/cstdio:42:10: fatal error: stdio.h: No such file or directory\n",
      "     #include <stdio.h>\n",
      "              ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/logging.o] Error 1\n",
      "    In file included from include/xgboost/data.h:10:0,\n",
      "                     from src/c_api/c_api.cc:3:\n",
      "    dmlc-core/include/dmlc/base.h:203:10: fatal error: sys/types.h: No such file or directory\n",
      "     #include <sys/types.h>\n",
      "              ^~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    g++-7 -c -DDMLC_LOG_CUSTOMIZE=1 -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP src/learner.cc -o build/learner.o\n",
      "    make: *** [build/c_api/c_api.o] Error 1\n",
      "    In file included from dmlc-core/include/dmlc/io.h:8:0,\n",
      "                     from src/learner.cc:7:\n",
      "    /usr/local/Cellar/gcc/7.3.0/include/c++/7.3.0/cstdio:42:10: fatal error: stdio.h: No such file or directory\n",
      "     #include <stdio.h>\n",
      "              ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make: *** [build/learner.o] Error 1\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/setup.py\", line 42, in <module>\n",
      "        LIB_PATH = libpath['find_lib_path']()\n",
      "      File \"/private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/xgboost/libpath.py\", line 49, in find_lib_path\n",
      "        'List of candidates:\\n' + ('\\n'.join(dll_path)))\n",
      "    XGBoostLibraryNotFound: Cannot find XGBoost Library in the candidate path, did you install compilers and run build.sh in root path?\n",
      "    List of candidates:\n",
      "    /private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/xgboost/libxgboost.dylib\n",
      "    /private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/xgboost/../../lib/libxgboost.dylib\n",
      "    /private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/xgboost/./lib/libxgboost.dylib\n",
      "    /Users/Thomas/anaconda3/envs/hello-tf/xgboost/libxgboost.dylib\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/n7/bb63f7vn6tn5cmm4nzxmrq7h0000gn/T/pip-install-x7c5wzix/xgboost/\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7e4ea9397687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step includes specifying the parameters to tune. You can refer\n",
    "to the official documentation to see all the parameters to tune. For the\n",
    "sake of the tutorial, you only choose two hyperparameters with two\n",
    "values each. XGBoost takes lots of time to train, the more\n",
    "hyperparameters in the grid, the longer time you need to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'xgbclassifier__gamma': [0.5, 1],\n",
    "        'xgbclassifier__max_depth': [3, 4]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You construct a new pipeline with XGBoost classifier. You choose to\n",
    "define 600 estimators. Note that `n_estimators` are a parameter that you\n",
    "can tune. A high value can lead to overfitting. You can try by yourself\n",
    "different values but be aware it can takes hours. You use the default\n",
    "value for the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = make_pipeline(\n",
    "    preprocess,\n",
    "    xgboost.XGBClassifier(\n",
    "                          n_estimators=600,\n",
    "                          objective='binary:logistic',\n",
    "                          silent=True,\n",
    "                          nthread=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve the cross-validation with the Stratified K-Folds\n",
    "cross-validator. You construct only three folds here to faster the\n",
    "computation but lowering the quality. Increase this value to 5 or 10 at\n",
    "home to improve the results.\n",
    "\n",
    "You choose to train the model over four iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3,\n",
    "                      shuffle = True,\n",
    "                      random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(model_xgb,\n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=4,\n",
    "                                   scoring='accuracy',\n",
    "                                   n_jobs=4,\n",
    "                                   cv=skf.split(X_train, y_train),\n",
    "                                   verbose=3,\n",
    "                                   random_state=1001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomized search is ready to use, you can train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_xgb = GridSearchCV(model_xgb, params, cv=10, iid=False)\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, XGBoost has a better score than the previous logisitc\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameter\", random_search.best_params_)\n",
    "print(\"best logistic regression from grid search: %f\" % random_search.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DNN - `MLPClassifier`\n",
    "\n",
    "\n",
    "Finally, you can train a deep learning algorithm with scikit-learn. The\n",
    "method is the same as the other classifier. The classifier is available\n",
    "at `MLPClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define the following deep learning algorithm:\n",
    "\n",
    "-   Adam solver\n",
    "\n",
    "-   Relu activation function\n",
    "\n",
    "-   Alpha = 0.0001\n",
    "\n",
    "-   batch size of 150\n",
    "\n",
    "-   Two hidden layers with 100 and 50 neurons respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn = make_pipeline(\n",
    "    preprocess,\n",
    "    MLPClassifier(solver='adam',\n",
    "                  alpha=0.0001,\n",
    "                  activation='relu',\n",
    "                    batch_size=150,\n",
    "                    hidden_layer_sizes=(200, 100),\n",
    "                    random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the number of layers to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn.fit(X_train, y_train)\n",
    "print(\"DNN regression score: %f\" % model_dnn.score(X_test, y_test))\n",
    "\n",
    "DNN regression score: 0.821253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME\n",
    "\n",
    "\n",
    "Now that you have a good model, you need a tool to trust it. Machine\n",
    "learning algorithm, especially random forest and neural network, are\n",
    "known to be blax-box algorithm. Say differently, it works but no one\n",
    "knows why.\n",
    "\n",
    "Three researchers have come up with a great tool to see how the computer\n",
    "makes a prediction. The paper is called `Why Should I Trust You?`\n",
    "\n",
    "They developed an algorithm named **Local Interpretable Model-Agnostic\n",
    "Explanations (LIME)**.\n",
    "\n",
    "Take an example:\n",
    "\n",
    "sometimes you do not know if you can trust a machine-learning\n",
    "prediction:\n",
    "\n",
    "A doctor, for example, cannot trust a diagnosis just because a computer\n",
    "said so. You also need to know if you can trust the model before putting\n",
    "it into production.\n",
    "\n",
    "Imagine we can understand why any classifier is making a prediction even\n",
    "incredibly complicated models such as neural networks, random forests or\n",
    "svms with any kernel\n",
    "\n",
    "will become more accessible to trust a prediction if we can understand\n",
    "the reasons behind it. From the example with the doctor, if the model\n",
    "told him which symptoms are essential you would trust it, it is also\n",
    "easier to figure out if you should not trust the model.\n",
    "\n",
    "Lime can tell you what features affect the decisions of the classifier\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "They are a couple of things you need to change to run LIME with python.\n",
    "First of all, you need to install lime in the terminal. You can use\n",
    "`pip install lime`\n",
    "\n",
    "Lime makes use of `LimeTabularExplainer` object to approximate the model\n",
    "locally. This object requires:\n",
    "\n",
    "-   a dataset in numpy format\n",
    "\n",
    "-   The name of the features: `feature_names`\n",
    "\n",
    "-   The name of the classes: `class_names`\n",
    "\n",
    "-   The index of the column of the categorical features:\n",
    "    `categorical_features`\n",
    "\n",
    "-   The name of the group for each categorical features:\n",
    "    `categorical_names`\n",
    "\n",
    "**Create numpy train set**\n",
    "\n",
    "You can copy and convert `df_train` from pandas to numpy very easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)\n",
    "# Create numpy data\n",
    "df_lime = df_train\n",
    "df_lime.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the class name** The label is accessible with the object\n",
    "`unique()`. You should see:\n",
    "\n",
    "-   '&lt;=50K'\n",
    "\n",
    "-   '&gt;50K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class name\n",
    "class_names = df_lime.label.unique()\n",
    "class_names\n",
    "\n",
    "array(['<=50K', '>50K'], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**index of the column of the categorical features**\n",
    "\n",
    "You can use the method you lean before to get the name of the group. You\n",
    "encode the label with `LabelEncoder`. You repeat the operation on all\n",
    "the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import sklearn.preprocessing as preprocessing\n",
    "categorical_names = {}\n",
    "for feature in CATE_FEATURES:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df_lime[feature])\n",
    "    df_lime[feature] = le.transform(df_lime[feature])\n",
    "    categorical_names[feature] = le.classes_\n",
    "print(categorical_names)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is ready, you can construct the different dataset.\n",
    "You actually transform the data outside of the pipeline in order to\n",
    "avoid errors with LIME. The training set in the `LimeTabularExplainer`\n",
    "should be a numpy array without string. With the method above, you have\n",
    "a training dataset already converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_lime, X_test_lime, y_train_lime, y_test_lime = train_test_split(df_lime[features],\n",
    "                                                    df_lime.label,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train_lime.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make the pipeline with the optimal parameters from XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = make_pipeline(\n",
    "    preprocess,\n",
    "    xgboost.XGBClassifier(max_depth = 3,\n",
    "                          gamma = 0.5,\n",
    "                          n_estimators=600,\n",
    "                          objective='binary:logistic',\n",
    "                          silent=True,\n",
    "                          nthread=1))\n",
    "\n",
    "model_xgb.fit(X_train_lime, y_train_lime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a warning. The warning explains that you do not need to create a\n",
    "label encoder before the pipeline. If you do not want to use LIME, you\n",
    "are fine to use the method from the first part of the tutorial.\n",
    "Otherwise, you can keep with this method, first create an encoded\n",
    "dataset, set get the hot one encoder within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best logistic regression from grid search: %f\" % model_xgb.score(X_test_lime, y_test_lime))\n",
    "\n",
    "best logistic regression from grid search: 0.873157\n",
    "\n",
    "model_xgb.predict_proba(X_test_lime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to use LIME in action, let's create a numpy array with the\n",
    "features of the wrong classification. You can use that list later to get\n",
    "an idea about what mislead the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([X_test_lime, y_test_lime], axis= 1)\n",
    "temp['predicted'] = model_xgb.predict(X_test_lime)\n",
    "temp['wrong']=  temp['label'] != temp['predicted']\n",
    "temp = temp.query('wrong==True').drop('wrong', axis=1)\n",
    "\n",
    "temp= temp.sort_values(by=['label'])\n",
    "temp.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create a lambda function to retrieve the prediction from the model\n",
    "with the new data. You will need it soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: model_xgb.predict_proba(x).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You convert the pandas dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lime = X_train_lime.values\n",
    "X_test_lime = X_test_lime.values\n",
    "\n",
    "X_test_lime\n",
    "\n",
    "model_xgb.predict_proba(X_test_lime)\n",
    "\n",
    "\n",
    "print(features,\n",
    "      class_names,\n",
    "      categorical_features,\n",
    "      categorical_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you create the `LimeTabularExplainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "### Train should be label encoded not one hot encoded\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_lime ,\n",
    "                                                   feature_names = features,\n",
    "                                                   class_names=class_names,\n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   categorical_names=categorical_names,\n",
    "                                                   kernel_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets choose a random household from the test set and see the model\n",
    "prediction and how the computer made his choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "i = 100\n",
    "print(y_test_lime.iloc[i])\n",
    "\n",
    "X_test_lime[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `explainer` with `explain_instance` to check the\n",
    "explanation behind the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(X_test_lime[i], predict_fn, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tensorflow/14_sci_kit_v4_files/image002.png\" >    \n",
    "\n",
    "We can see that the classifier predicted the household correctly. The\n",
    "income is, indeed, above 50k.\n",
    "\n",
    "The first thing we can say is the classifier is not that sure about the\n",
    "predicted probabilities. The machine predicts the household has an\n",
    "income over 50k with a probability of 64%. This 64% is made up of\n",
    "`Capital gain` and `marital`. The blue color contributes negatively to\n",
    "the positive class and the orange line, positively.\n",
    "\n",
    "The classifier is confused because the capital gain of this household is\n",
    "null, while the capital gain is usually a good predictor of wealth.\n",
    "Besides, the household works less than 40 hours per week. Age,\n",
    "occupation, and sex contribute positively to the classifier.\n",
    "\n",
    "If the marital status were single, the classifier would have predicted\n",
    "an income below 50k (0.64-0.18 = 0.46)\n",
    "\n",
    "We can try with another household which has been wrongly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head(3)\n",
    "\n",
    "temp.iloc[1,:-2]\n",
    "i = 1\n",
    "print('This observation is', temp.iloc[i,-2:])\n",
    "\n",
    "exp = explainer.explain_instance(temp.iloc[1,:-2], predict_fn, num_features=6)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tensorflow/14_sci_kit_v4_files/image003.png\" >  \n",
    "\n",
    "The classifier predicted an income below 50k while it is untrue. This\n",
    "household seems odd. It does not have a capital gain, nor capital loss.\n",
    "He is divorced and is 60 years old, and it is an educated people, i.e.,\n",
    "`education_num` &gt; 12. According to the overall pattern, this\n",
    "household should, like explain by the classifier, get an income below\n",
    "50k.\n",
    "\n",
    "You try to play around with LIME. You will notice gross mistakes from\n",
    "the classifier.\n",
    "\n",
    "You can check the GitHub of the owner of the library. They provide extra\n",
    "documentation for image and text classification.\n",
    "\n",
    "## Summary\n",
    "\n",
    "\n",
    "Below is a list of some useful command with scikit learn version\n",
    "\n",
    "\n",
    "| create train/test dataset                      | object                |\n",
    "|------------------------------------------------|-----------------------|\n",
    "| Build a pipeline                               |                       |\n",
    "| select the column and apply the transformation | makecolumntransformer |\n",
    "| type of transformation                         |                       |\n",
    "| standardize                                    | StandardScaler        |\n",
    "| min max                                        | MinMaxScaler          |\n",
    "| Normalize                                      | Normalizer            |\n",
    "| Impute missing value                           | Imputer               |\n",
    "| Convert categorical                            | OneHotEncoder         |\n",
    "| Fit and transform the data                     | fit_transform         |\n",
    "| Make the pipeline                              | make_pipeline         |\n",
    "| Basic model                                    |                       |\n",
    "| logistic regression                            | LogisticRegression    |\n",
    "| XGBoost                                        | XGBClassifier         |\n",
    "| Neural net                                     | MLPClassifier         |\n",
    "| Grid search                                    | GridSearchCV          |\n",
    "| Randomized search                              | RandomizedSearchCV    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
